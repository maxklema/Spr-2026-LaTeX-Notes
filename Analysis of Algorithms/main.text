\documentclass[11pt]{article}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=\dimexpr15mm+1.5\baselineskip,bottom=2cm]{geometry}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{fancyhdr} % Headings
\usepackage{listings} % Code
\usepackage{parskip} % Spacing for Paragraphs
\usepackage{mdframed} % Asides
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsmath}

% Asides
\newenvironment{aside}
  {\begin{mdframed}[style=0,%
      leftline=false,rightline=false,leftmargin=2em,rightmargin=2em,%
          innerleftmargin=0pt,innerrightmargin=0pt,linewidth=0.75pt,%
      skipabove=7pt,skipbelow=7pt]\small}
  {\end{mdframed}}

\renewcommand{\headrulewidth}{.2mm} % header line width

\rhead{\today}
\lhead{\textbf{Maxwell Klema}}
\cfoot{\thepage}


\title{Analysis of Algorithms}
\date{Spring 2026, Purdue University Fort Wayne}
\author{Maxwell Klema | Professor Zesheng Chen}

\lstset{
    frame=single, % Adds a single line frame around the code
}

\begin{document}

\maketitle
\pagestyle{fancy}


% The well known Pythagorean theorem \(x^2 + y^2 = z^2\) was 
% proved to be invalid for other exponents. 
% Meaning the next equation has no integer solutions:

% \[ x^n + y^n = z^n + log(n)\]
% \[\sqrt{x^2+1}\] 
% This is a simple math expression \(\sqrt{x^2+1}\) inside text. 
% And this is also the same: 
% \( \log_b a\)
% \begin{lstlisting}[language=Python]
%     dsad
% \end{lstlisting}


\section{Lecture 1 - Algorithms Introduction}
\vspace{11pt}
\textbf{Merge Sort} is sort of like the "hello world" for advanced algorithms. In a merge sort, you keep splitting the array in half until there are only chunks of one element, then you merge back up. This means that you take two single elements and merge them from smallest to largest. Then, you take that shorted chunk of some size \textit{m}, and merge it with another sorted array of some size \textit{m}, until the whole array with \textit{n} elements is sorted.

A good example to try a merge sort on is \textit{[5, 4, 1, 8, 7, 2, 6, 3]}.

\vspace{11pt}
\subsection{Why Study Algorithms?}
\vspace{11pt}

\textbf{Review: What is an Algorithm?} It's a set of well-defined rules - a recipe, in effect - for solving some computation problem.

Example: Sorting, shortest path, and scheduling.

It is important to study algorithms because it applies to all other branches of computer science. For example, in Networking, there are routing protocols such as Dijkstra's algorithm. There is intra-domain and inter-domain routing which may use different routing algorithms. In Computer Security, there are many cryptography algorithms. In Computer Graphics, there are geometric algorithms. In databases, there are balance search tree algorithms to speed up database queries using indexing. In Computational Biology, dynamic programming is heavily used. In Machine Learning, any model, including deep learning models, is a set of algorithms. An example is a clustering algorithm.

Studying algorithms plays a key role in modern technological innovation.
\vspace{11pt}
\begin{aside}
"Everyone knows Moore's Law - a prediction made in 1965 by Intel c-ofounder Gordon Moore that the density of transistors in integrated circuits would continue to double every one to two years... in many areas, performance gains due to improvements in algorithms have vastly exceeded even the dramatic performance gains due to increased processor speed."

- Excerpt from Report to the President and Congress: Designing a Digital Future, December 2010 (page 71).
\end{aside}
\vspace{11pt}
Another famous use-case of algorithms is Google's PageRank algorithm, which measures the "quality" of a webpage based on how many other pages refer to it, as well as other quality metrics. This algorithm relies on probability distribution using eigenvectors and matrices.

Additionally, studying algorithms provides a novel "lens" on processes outside of computer science and technology. This includes quantum mechanics, economic markets, and evolution.

Studying algorithms is also challenging. But, that makes you a better programmer and an overall better problem-solver. It can also be fun: seeing an abstract problem and coming up with a creative solution. It is the blend of precision and creativity.

\subsection{Integer Multiplication and Karatsuba}
\vspace{11pt}
\subsubsection{Integer Multiplication Problem}
\textbf{Input}: 2 n-digit numbers $x$ and $y$.

\textbf{Output}: product of $x * y$.
\vspace{11pt}
\\A "primitive operation" may be adding or multiplying 2 single-digit numbers, and adding a zero to the beginning or end of a number.

For example, $12 + 8$ takes two operations: $8 + 2$ and $1 + 1$.


\vspace{11pt}
\textbf{The Grade-School Algorithm}
\vspace{11pt}

Let's calculate $5678 * 1234$. Because it is O(\(6n^2\)) (double-check), it should take a maximum of \texttt{\~}96 operations to solve.

\vspace{11pt}
\begin{aside}
    \textbf{Note}: To get O(\(6n^2\)), it is important to understand the the multiplication phase takes O(\(3n^2)\) operations (the multiply, the addition of the carry from the previous digit in the same row, and the update that calculates the new carry to pass to the new digit to the left). The addition phase, similarly, takes O(\(3n^2\)) operations (the addition in the column, the addition of the carry-in that came from the column to the right, and the calculation of the carry-out to the column on the left).
\end{aside}
\vspace{11pt}

However, that is pretty bad. Can we do better than the "obvious" or "bruteforce" method?

What if we take a divide-and-conquer approach? We divide each number into half and then half and then half, i.e. recursively divide n-digit number into half until it is a single digit and then merge it back into the actual solution.

For example, a = 56, b = 78, c = 12, d = 34. 

Let $x = {10^{n/2}} * a + b$. 

Let $y = {10^{n/2}} * c + d$. 

Finally, $x * y = ({10^n} * ac) + {10^{n/2}}(ad+bc) + bd$

$x*y$ contains $\{ac, ad, bc, bd\}$. These four items are still integer multiplication. For each of these, you apply $x * y$ and get another $4$ items (like a tree). Then you merge them back.

n $\to$ {n/2} $\to$ {n/4} $\to$ ... $\to$ {1 digit}

However, this algorithm does not do better! Why? Because, eventually, the time complexity is O(\(n^2\)).
\vspace{11pt}
\begin{aside}
    The recursive relation of the problem can be expressed as $T(n) = 4T({n/2}) + O(n)$, where O(n) is the work needed to "merge" the results (adding the products and shifting digits by adding zeros). Four is the number of smaller multiplications to perform, while {n/2} shows that each of the smaller multiplications deals with numbers half the size of the original.

    \(log_24 = 2\) shows that each additional level does 2 times more work. By the time you reach 1-digit numbers, you perform exactly \(n^2\) multiplications - the same as grade-school.

    Therefore, O(\(n^{log_24})\)) = O(\(n^2)\).
\end{aside}
\vspace{11pt}
\subsubsection{Karatsuba Multiplication}

So, how does Karatsuba Multiplication fix this?

(a+b)*(c+d) = ac + ad + bc + bd, then subtract - ac - bd = ad + bc. \textbf{You can reduce four recursive calls into three}. That's certaintly better than third-grade multiplication.

The three recursive calls are: $ac$, $bd$, $(a+b) * (c+d)$, then $sum(ad + bc)$ becomes the third.

The new recursive formula becomes:

$T(n) = 3T({n/2) + O(n)}$

Thus, the new additional work factor is only \(log_23 = 1.58\), showing that each level does 1.58 times more additional work than the last, making the overall time complexity \textbf{O(\(n^{log_23})\)}.

For large n-digit multiplications, they use Karatsuba Multiplication.

\vspace{11pt}
\subsection{Merge Sort}

Merge sort is a good introduction to divide-and-conquer algorithms. It improves over selection, insertion, and bubble sorts. These are all O(\(n^2\)).

It helps to calibrate your preparation and motivates guiding principles for algorithm analysis (worst-case and asymptotic analysis).

Analysis generalizes to "Master Method".

\subsubsection{The Sorting Problem}

\textbf{Input:} array of n numbers, unsorted.

\textbf{Output:} Same numbers, sorted in increasing order.
\vspace{11pt}
\begin{aside}
    \textbf{Note}: For now, we assume that all elements are distinct.
\end{aside}
\vspace{11pt}
\subsubsection{Merge Sort: Example}
\vspace{11pt}
\textit{[5,4,1,8,7,2,6,3]} $\to$ \textit{[5,4,1,8] \& [7,2,6,3]} $\to$ ... (recursion) ... $\to$ \textit{[1,4,5,8] \& [2,3,6,7]} $\to$ \textit{[1,2,3,4,5,6,7,8]}

\vspace{11pt}
\subsubsection{Merge Sort: Pseudocode}

Three function calls: Recursively sort 1st half of the input array, recursively sort 2nd half of the input array, and merge two sorted sublists into one.

The base case for merge sort is when there is only one element or there are zero elements (where n \% 2 != 0).

C = output[length = n]

A = 1st sorted array [n/2]

B = 2nd sorted array [n/2]

i = 1
\\
j = 1
\vspace{11pt}
\begin{aside}
    \textbf{Note:} we assume the starting point is 1 instead of 0.
\end{aside}
\vspace{11pt}

\begin{lstlisting}[language=Python]
for k = 1 to n  
    if A(i) < B(j)
        C(k) = A(i)
        i++
    else [B(j) < A(i)]
        C(k) = B(j)
        j++

\end{lstlisting}
\vspace{11pt}
\vspace{11pt}
\begin{aside}
    \textbf{Note:} This is just the pseudo code for merging, which is O(m). Where m $\leq$ n.
\end{aside}
\vspace{11pt}

\textbf{Running Time of Merge Sort}
\vspace{11pt}

The time complexity is O(\(nlog(n)\)). It takes \(log_2n\) times to divide an array of size n by 2 to get 1 or 0 elements.

The running time is roughly \# amount of lines of code executed.

\textbf{Upshot:} the running time of \textbf{merge} in the array of m numbers is 4m + 2 $\leq$ 6m where m $\geq$ 1

\textbf{Claim:} Merge Sort requires $\leq$ \(6n*log_2n + 6n\)  operations to sort n numbers

\vspace{11pt}
\begin{aside}
    \textbf{Recall:} \(log_2n\) is the \# of times you divide by 2 until you get down to 1.
\end{aside}
\vspace{11pt}

\textbf{Proof of claim (assuming n = power of 2)}

Use a recursive tree:

level 0 $\to$ root (outer call to Merge Sort) [\(2^1 - 1 = 1\) cumulative divisions]

level 1 $\to$ 1st recursive call [\(2^2 - 1 = 3\) cumulative divisions]

level 2 $\to$ 2nd recursive call [\(2^3 - 1 = 7\) cumulative divisions]

...

\vspace{11pt}
\begin{aside}
   \textbf{Quiz:} Roughly how many levels does this recursion tree have (as a function of $n$, the length of the input array)?

   $\to$ \(log_2n\), well actually \(log_2n + 1\) for the root level.
\end{aside}
\vspace{11pt}

\vspace{11pt}
\begin{aside}
   \textbf{Quiz:} What is the pattern? Fill in the blanks in the following statement: at each level j = 0,1,2,.., \(log_2n\), there are <blank> subproblems, each of size <blank>.

   $\to$ \(2^j\) subproblems \& each the size of \(n / 2^j\).

   \textbf{Proof of claim (assuming n = power of 2):} 
   
   $\leq$ \(2^j * 6(n/2^j) = 6n\), where $6$ is the work per level-j subproblem.

   If there is $6n$ work per level, and there are \(log_2n + 1\) levels, then the total Big-O time complexity is:

   O(\(6n(log_2n + 1)\))
\end{aside}
\vspace{11pt}

\subsection{Guiding Principles for Analysis of Algorithms}
\vspace{11pt}
\textbf{Guiding Principle \#1}
\vspace{11pt}

"Worst-case analysis": Our running time bound holds for every input of length n. This is particularly appropriate for "general-purpose" routines, as opposed to "average-case" analysis and benchmarks.

In fact, the worst-case analysis is usually easier to analyze, making our job easier.

\vspace{11pt}
\textbf{Guiding Principle \#2}
\vspace{11pt}

Constant factors and lower-order terms do not need to be paid much attention to. The justifications for this include making algorithms easier to analyze, the fact that constants depend on architecture, compiler, and/or programmer anyways, and that the reduction of constants lose only very little predictive power (as we will see).

\vspace{11pt}
\textbf{Guiding Principle \#3}
\vspace{11pt}

\textbf{Asymptotic Analysis:} Focus on running time for large input size $n$.

\vspace{11pt}
\begin{aside}
    \textbf{For example}: \(6nlog_2n + 6n\) [merge sort] may be worse than \((1/2)n^2\) [insertion sort] for very small n. This can lead to wrong conclusions that insertion sort is better, but at a small enough n, the difference in performance is extremely minimal (nanoseconds).
\end{aside}
\vspace{11pt}

Really, only big problems with large n are interesting, as different Big-O functions involving n have different growth rates.

\vspace{11pt}
\includegraphics[width=0.7\textwidth]{1.png}

\vspace{11pt}
\includegraphics[width=0.7\textwidth]{2.png}

\vspace{11pt}
\textbf{What is a "Fast" Algorithm?}
\vspace{11pt}

A fast algorithm is one where the worst case running time grows slowly with input size. An example being O(\(log_2n\)) or even O(1), although that is rare.

\vspace{11pt}
\begin{aside}
    A good goal is usually keeping an algorithm as close to \(O(n)\) as possible.
\end{aside}
\vspace{11pt}

\vspace{11pt}
\textbf{For-Free Primitives}
\vspace{11pt}

An algorithm with a linear or near-linear running time acts as a primitive that can be used essentially "for free". In other words, the amount of time required barely exceeds what you need to read the input.

An example of this can be sorting. It is important to put these for-free primitive algorithms into your toolbox.

\newpage
\section{Lecture 2 - Asymptotic Notation}
\vspace{11pt}

In the last lecture, we talked about the Integer multiplication problem (x*y). However, using traditional methods, for an n-digit number, the runtime is O($n^2$). Can we do better?

Can we use divide-and-conquer? What about $n/2 \to n/4 \to ...\to 1$.

For example, say $x=5678, y=1234$, then cut in half such that $a=56,b=78,c=12,d=34$.

Then, $x=100a+b, y=100c+d$

$x*y=(100a+b)(100c+d)$.

$= 10^4ac + 100ad + 100bc + bd$. Now, given this, we are turning this into 4 $n/2$ digit product. We can keep going recursively by dividing in half. However, this does not make life easier. In fact, the running time complexity is still $O(n^2)$.

If we look at $(a+b)(c+d)=ac+ad+bc+bd$. We can get $(ad+bc)$ using $ac+ad+bc+bd - ac - bd = ad+bc$. Now, instead of four operations per level, we can do three operations instead. It is 3 $n/2$ digit multiplication. This does better than the elementary school multiplication algorithm.

\vspace{11pt}
\subsection{Merge Sort}
\vspace{11pt}

We then began talking about \textbf{Merge Sort}. Given an array, we split the array in half and then recursively sort both halves. Once both halves are sorted, we then merge them together.

If we assume that there are $m$ total elements in the array, and each half has $m/2$ elements. For the merge, $2m \geq 2$ and the time complexity is $\leq 4m + 2m = 6m$.

In a recursive tree, we divide each node into two sub-arrays until we have single-element arrays. Each level has $2^j$ nodes per level. There are $n$ nodes at the end. Given that $j = log_2n$, then there are $log_2n+1$ levels. For each node (sub-array), there are $n/2^j$ nodes.

Then, for each level there are $n/2^j * 6 * 2^j$ steps. With $log_2n+1$ levels, the overall runtime complexity is $O(6nlog_2n + 6n)$.

\vspace{11pt}
\subsection{Asymptotic Notation}
\vspace{11pt}

The goal is to identify a sweet spot of granularity for reasoning about algorithms. We want to suppress second-order details like constant factors and lower-order terms and introduce Big-O notation.

In seven words, asymptotic notation \textbf{suppresses constant factors and lower-order terms}. Constant factors are too system-dependent. For example, a programming language, architecture, or compiler. Additionally, lower-order terms are irrelevant for large inputs. For example, having $3n^2+2n$, we can just write it as $O(n^2)$. The "big-O notation" buckets algorithms into groups according to their asymptotic worst-case running time.

\vspace{11pt}
\textbf{Examples:}

Input: array A of n integers, and an integer t. Output is whether or not A contains t. $\to$ $O(n)$

Input: arrays A and B of n integers each, and an integer t. The output whether or not A or B contains t. $\to$ $O(n)$

Input: arrays of A and B of n integers each. Output: Whether or not there is an integer t contained in both A and B. $\to$ $O(n^2)$

Input: an array of A of n integers. Output: Whether or not A contains an integer more than once. Note, assume that we are using two for loops. $\to$ $O(n^2)$. Another way is $n$ choose $2$ permutations which equates to $\frac{n(n-1)}{2}$.

\vspace{11pt}
\subsection{Big-O Notation [Asymptotic Upper Bound]}
\vspace{11pt}

Let $T(n)=$ function on $n = 1, 2, 3, ...$ [usually, the worst-case running time of an algorithm, or the real running time of an algorithm]. A big question is \textbf{when is $T(n)=O(f(n))$?} If eventually (for all sufficiently large $n$), $T(n)$ is bounded above by a constant multiple of $f(n)$.

In other words, $O(f(n))$ is a way to categorize or "bound" the runtime. It describes the "shape" of the growth as $n$ becomes very large.
\vspace{11pt}

The formal definition states that $T(n)=O(f(n))$ if and only if there exists constants such that $c, n_0 > 0$:

\vspace{11pt}
\begin{center}
    $T(n) \leq c * f(n)$
\end{center}
\vspace{11pt}

for all $n \geq n_0$. Also $c, n_0$ cannot depend on $n$.

\vspace{11pt}
\begin{aside}
    \textbf{Note:} $c$ is the constant multiple (positive) that scales the function $f(n)$. Its purpose allows us to ignore specific implementation details like hardware speed or compiler optimizations. It matters because even if an algorithm takes $3n^2$ steps, we can say it is $O(n^2)$ because we can just pick $c=3$ (or any higher number) to satisfy the inequality. It focuses on the analysis on the rate of growth rather than the exact number of operations.

    Meanwhile, $n_0$ is the threshold or starting point. It is the value of the input size $n$ where the upper bound starts to kick in. Big-O notation is interested in asymptotic behavior, meaning what happens as $n$ grows to infinity. For very small inputs, an algorithm might be slow due to initialization or overhead (e.g., 100 + n is larger than $n^2$ when $n=2$). The $n_0$ value says to ignore those small cases. Eventually, $c*f(n)$ will always be greater than or equal to $T(n)$ for every value of $n$ from this point onward.
\end{aside}
\vspace{11pt}

\textbf{A Picture Illustrating When $T(n)=O(f(n))$}

\vspace{11pt}
\begin{center}
    \includegraphics[width=0.5\textwidth]{3.png}
\end{center}
\vspace{11pt}

\newpage
Note, in the image above, $3*f(n)$ is used as an upper bound to show that $T(n)=O(f(n))$.

\vspace{11pt}
\textbf{Example 1:}

Claim: If $T(n)=a_kn^k+\dots+a_1n+a_0$, then $T(n)=O(n^k)$

Proof: Choose $n_0=1$ and $c=|a_k| + |a_{k-1}| + \dots + |a_1| + |a_0|$. We need to show that $\forall n \geq 1,T(n) \leq c*n^k$.

We have, for every $n \geq 1$:

\vspace{11pt}
\begin{center}
$T(n) \leq |a_k|n^k + \dots + |a_{1}|n+|a_0|$

$\leq |a_k|n^k + \dots + |a_1|n^k + |a_0|$

$= c*n^k$
\end{center}

\vspace{11pt}
\textbf{Example 2:}

Claim: For every $k \geq 1$, $n^k$ is not $O(n^{k-1})$

Proof: By contradiction. Suppose $n^k=O(n^{k-1})$. Then, there exists constants $c,n_0$ such that:

\vspace{11pt}
\begin{center}
    $n^k \leq c*n^{k-1}$ for $\forall n \geq n_0$

    But then [canceling $n^{k-1}$ from both sides]:

    $n \leq c$ for $\forall n \geq n_0$

    Which is clearly false [Contradiction]
\end{center}

\vspace{11pt}
\textbf{Example 3:}

Claim: $2^{n+10}=O(2^n)$

Proof: We need to pick constants $c,n_0$ such that:

\vspace{11pt}
\begin{center}
    (*) $2^{n+10} \leq c * 2^n$ for $\forall n \geq n_0$

    Note: $2^{n+10}=2^{10}*2^n=(1024)*2^n$.
\end{center}
\vspace{11pt}

So, if we choose $c \geq 1024,n_0=1$, then (*) holds.

\vspace{11pt}
\textbf{Example 4:}

Claim: $2^{10n} \neq O(2^n)$

Proof: By contradiction, if $2^{10n}=O(2^n)$ then there exist constants $c,n_0 > 0$ such that:

\vspace{11pt}
\begin{center}
    $2^{10n} \leq c*2^n$ for $n \geq n_0$

    But then [canceling $2^n$]:

    $2^{9n} \leq c$ for $\forall n \geq n_0$,

    which is certainly false.
\end{center}
\vspace{11pt}

\newpage
\subsection{Big-Omega Notation [Asymptotic Lower Bound]}
\vspace{11pt}

The formal definition is $T(n)=\Omega(f(n))$ if and only if there exists constants $c,n_0 > 0$ such that:

\vspace{11pt}
\begin{center}
    $T(n) \geq c *f(n)$ for $\forall n \geq n_0$
\end{center}
\vspace{11pt}

In other words, the inequality $T(n) \geq c * f(n)$ means that for all sufficiently large inputs $(n \geq n_0)$, the actual runtime $T(n)$ will always be greater than or equal to a constant multiple of $f(n)$. This implies that the algorithm will never run faster than that bound once the input size is large enough.

Again, $c,n_0$ cannot depend on $n$.

\vspace{11pt}
\subsection{Big-Theta Notation [Asymptotic Tight Bound]}
\vspace{11pt}

The formal definition is $T(n)=\theta(f(n))$ if and only if there exist constants $c_1,c_2,n_0$ such that:

\vspace{11pt}
\begin{center}
    $c_1*f(n)\leq T(n) \leq c_2 *f(n)$ for $\forall n \geq n_0$
\end{center}
\vspace{11pt}

In other words, $T(n)=\theta(f(n))$ does not specify an average running time, but rather a tight bound for whatever runtime function $(T(n))$ is being analyzed. $c_1*f(n)$ acts as the lower bound (the floor): The function cannot grow any slower than this. Meanwhile, $c_2*f(n)$ acts as the upper bound (ceiling): The function cannot grow any faster than this.

Again, $c_1,c_2,n_0$ cannot depend on $n$.

Mean: $T(n)=O(f(n))$ ($(T(n)$ does not grow faster than $f(n)$) and $T(n)=\Omega(f(n))$ ($T(n)$ does not grow slower than $f(n)$).

\vspace{11pt}
\textbf{A Quiz}
\vspace{11pt}

Let $T(n)=\frac12n^2+3n$. Which of the following statements are true? (There might be more than one correct answer.)
\begin{itemize}
    \item $T(n)=O(n)$. False! $\frac{1}{2}n+3 \not\leq c$.
    \item $T(n)=\Omega(n)$. True! $\frac12n+3 \geq c$. A quadratic function always grows faster than a linear one. Therefore, the quadratic function will always be "at least" as large as a linear function for large values of $n$, $n^2$ is bounded below by $n$.
    \item $T(n)=\Theta(n^2)$. True! $c_1 \leq \frac12+\frac3n \leq c_2$. To prove this, $T(n)=\Omega(n^2),T(n)=O(n^2)$.\textbf{ The function must satisfy the function using the same $f(n)$.}
    \item $T(n)=O(n^3)$. True! $\frac12+\frac3n \leq c*n$.
\end{itemize}
\vspace{11pt}

\textbf{Example on Big-Theta Notation}

Claim: for every pair of positive function $f(n)$ and $g(n)$:

\vspace{11pt}
\begin{center}
    $\max\{f,g\}=\theta(f(n)+g(n))$
\end{center}
\vspace{11pt}

Proof: For every $n$, we have:

\vspace{11pt}
\begin{center}
    $\max\{f(n),g(n)\} \leq f(n)+g(n) $
\end{center}
\vspace{11pt}

and

\vspace{11pt}
\begin{center}
    $2*\max\{f(n),g(n)\} \geq f(n)+g(n) = \max\{f(n),g(n)\} \geq \frac12(f(n)+g(n))$
\end{center}
\vspace{11pt}

Thus

\vspace{11pt}
\begin{center}
    $\frac12 * (f(n)+g(n)) \leq \max\{f(n),g(n)\} \leq f(n)+g(n)$ for $\forall n > 1$

    $=> \max\{f,g\} = \theta(f(n)+g(n))$ [where $n_0=1,c_1=\frac12,c_2=1$]
\end{center}
\vspace{11pt}

\textbf{Little-O Notation [Mathematical Version]}
\vspace{11pt}

Definition: $T(n)=o(f(n))$ if and only if for all constants $c>0$, there exists a constant $n_0$ such that:

\vspace{11pt}
\begin{center}
    $T(n) \leq c * f(n)$ for $\forall n \geq n_0$
\end{center}
\vspace{11pt}

Exercise: $\forall k \geq1,n^{k-1}=o(n^k)$






\end{document}
