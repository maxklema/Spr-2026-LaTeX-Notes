\documentclass[11pt]{article}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=\dimexpr15mm+1.5\baselineskip,bottom=2cm]{geometry}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{fancyhdr} % Headings
\usepackage{listings} % Code
\usepackage{parskip} % Spacing for Paragraphs
\usepackage{mdframed} % Asides
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsmath}

% Asides
\newenvironment{aside}
  {\begin{mdframed}[style=0,%
      leftline=false,rightline=false,leftmargin=2em,rightmargin=2em,%
          innerleftmargin=0pt,innerrightmargin=0pt,linewidth=0.75pt,%
      skipabove=7pt,skipbelow=7pt]\small}
  {\end{mdframed}}

\renewcommand{\headrulewidth}{.2mm} % header line width

\rhead{\today}
\lhead{\textbf{Maxwell Klema}}
\cfoot{\thepage}


\title{Analysis of Algorithms}
\date{Spring 2026, Purdue University Fort Wayne}
\author{Maxwell Klema | Professor Zesheng Chen}

\lstset{
    frame=single, % Adds a single line frame around the code
}

\begin{document}

\maketitle
\pagestyle{fancy}


% The well known Pythagorean theorem \(x^2 + y^2 = z^2\) was 
% proved to be invalid for other exponents. 
% Meaning the next equation has no integer solutions:

% \[ x^n + y^n = z^n + log(n)\]
% \[\sqrt{x^2+1}\] 
% This is a simple math expression \(\sqrt{x^2+1}\) inside text. 
% And this is also the same: 
% \( \log_b a\)
% \begin{lstlisting}[language=Python]
%     dsad
% \end{lstlisting}


\section{Lecture 1 - Algorithms Introduction}
\vspace{11pt}
\textbf{Merge Sort} is sort of like the "hello world" for advanced algorithms. In a merge sort, you keep splitting the array in half until there are only chunks of one element, then you merge back up. This means that you take two single elements and merge them from smallest to largest. Then, you take that shorted chunk of some size \textit{m}, and merge it with another sorted array of some size \textit{m}, until the whole array with \textit{n} elements is sorted.

A good example to try a merge sort on is \textit{[5, 4, 1, 8, 7, 2, 6, 3]}.

\vspace{11pt}
\subsection{Why Study Algorithms?}
\vspace{11pt}

\textbf{Review: What is an Algorithm?} It's a set of well-defined rules - a recipe, in effect - for solving some computation problem.

Example: Sorting, shortest path, and scheduling.

It is important to study algorithms because it applies to all other branches of computer science. For example, in Networking, there are routing protocols such as Dijkstra's algorithm. There is intra-domain and inter-domain routing which may use different routing algorithms. In Computer Security, there are many cryptography algorithms. In Computer Graphics, there are geometric algorithms. In databases, there are balance search tree algorithms to speed up database queries using indexing. In Computational Biology, dynamic programming is heavily used. In Machine Learning, any model, including deep learning models, is a set of algorithms. An example is a clustering algorithm.

Studying algorithms plays a key role in modern technological innovation.
\vspace{11pt}
\begin{aside}
"Everyone knows Moore's Law - a prediction made in 1965 by Intel c-ofounder Gordon Moore that the density of transistors in integrated circuits would continue to double every one to two years... in many areas, performance gains due to improvements in algorithms have vastly exceeded even the dramatic performance gains due to increased processor speed."

- Excerpt from Report to the President and Congress: Designing a Digital Future, December 2010 (page 71).
\end{aside}
\vspace{11pt}
Another famous use-case of algorithms is Google's PageRank algorithm, which measures the "quality" of a webpage based on how many other pages refer to it, as well as other quality metrics. This algorithm relies on probability distribution using eigenvectors and matrices.

Additionally, studying algorithms provides a novel "lens" on processes outside of computer science and technology. This includes quantum mechanics, economic markets, and evolution.

Studying algorithms is also challenging. But, that makes you a better programmer and an overall better problem-solver. It can also be fun: seeing an abstract problem and coming up with a creative solution. It is the blend of precision and creativity.

\subsection{Integer Multiplication and Karatsuba}
\vspace{11pt}
\subsubsection{Integer Multiplication Problem}
\textbf{Input}: 2 n-digit numbers $x$ and $y$.

\textbf{Output}: product of $x * y$.
\vspace{11pt}
\\A "primitive operation" may be adding or multiplying 2 single-digit numbers, and adding a zero to the beginning or end of a number.

For example, $12 + 8$ takes two operations: $8 + 2$ and $1 + 1$.


\vspace{11pt}
\textbf{The Grade-School Algorithm}
\vspace{11pt}

Let's calculate $5678 * 1234$. Because it is O(\(6n^2\)) (double-check), it should take a maximum of \texttt{\~}96 operations to solve.

\vspace{11pt}
\begin{aside}
    \textbf{Note}: To get O(\(6n^2\)), it is important to understand the the multiplication phase takes O(\(3n^2)\) operations (the multiply, the addition of the carry from the previous digit in the same row, and the update that calculates the new carry to pass to the new digit to the left). The addition phase, similarly, takes O(\(3n^2\)) operations (the addition in the column, the addition of the carry-in that came from the column to the right, and the calculation of the carry-out to the column on the left).
\end{aside}
\vspace{11pt}

However, that is pretty bad. Can we do better than the "obvious" or "bruteforce" method?

What if we take a divide-and-conquer approach? We divide each number into half and then half and then half, i.e. recursively divide n-digit number into half until it is a single digit and then merge it back into the actual solution.

For example, a = 56, b = 78, c = 12, d = 34. 

Let $x = {10^{n/2}} * a + b$. 

Let $y = {10^{n/2}} * c + d$. 

Finally, $x * y = ({10^n} * ac) + {10^{n/2}}(ad+bc) + bd$

$x*y$ contains $\{ac, ad, bc, bd\}$. These four items are still integer multiplication. For each of these, you apply $x * y$ and get another $4$ items (like a tree). Then you merge them back.

n $\to$ {n/2} $\to$ {n/4} $\to$ ... $\to$ {1 digit}

However, this algorithm does not do better! Why? Because, eventually, the time complexity is O(\(n^2\)).
\vspace{11pt}
\begin{aside}
    The recursive relation of the problem can be expressed as $T(n) = 4T({n/2}) + O(n)$, where O(n) is the work needed to "merge" the results (adding the products and shifting digits by adding zeros). Four is the number of smaller multiplications to perform, while {n/2} shows that each of the smaller multiplications deals with numbers half the size of the original.

    \(log_24 = 2\) shows that each additional level does 2 times more work. By the time you reach 1-digit numbers, you perform exactly \(n^2\) multiplications - the same as grade-school.

    Therefore, O(\(n^{log_24})\)) = O(\(n^2)\).
\end{aside}
\vspace{11pt}
\subsubsection{Karatsuba Multiplication}

So, how does Karatsuba Multiplication fix this?

(a+b)*(c+d) = ac + ad + bc + bd, then subtract - ac - bd = ad + bc. \textbf{You can reduce four recursive calls into three}. That's certaintly better than third-grade multiplication.

The three recursive calls are: $ac$, $bd$, $(a+b) * (c+d)$, then $sum(ad + bc)$ becomes the third.

The new recursive formula becomes:

$T(n) = 3T({n/2) + O(n)}$

Thus, the new additional work factor is only \(log_23 = 1.58\), showing that each level does 1.58 times more additional work than the last, making the overall time complexity \textbf{O(\(n^{log_23})\)}.

For large n-digit multiplications, they use Karatsuba Multiplication.

\vspace{11pt}
\subsection{Merge Sort}

Merge sort is a good introduction to divide-and-conquer algorithms. It improves over selection, insertion, and bubble sorts. These are all O(\(n^2\)).

It helps to calibrate your preparation and motivates guiding principles for algorithm analysis (worst-case and asymptotic analysis).

Analysis generalizes to "Master Method".

\subsubsection{The Sorting Problem}

\textbf{Input:} array of n numbers, unsorted.

\textbf{Output:} Same numbers, sorted in increasing order.
\vspace{11pt}
\begin{aside}
    \textbf{Note}: For now, we assume that all elements are distinct.
\end{aside}
\vspace{11pt}
\subsubsection{Merge Sort: Example}
\vspace{11pt}
\textit{[5,4,1,8,7,2,6,3]} $\to$ \textit{[5,4,1,8] \& [7,2,6,3]} $\to$ ... (recursion) ... $\to$ \textit{[1,4,5,8] \& [2,3,6,7]} $\to$ \textit{[1,2,3,4,5,6,7,8]}

\vspace{11pt}
\subsubsection{Merge Sort: Pseudocode}

Three function calls: Recursively sort 1st half of the input array, recursively sort 2nd half of the input array, and merge two sorted sublists into one.

The base case for merge sort is when there is only one element or there are zero elements (where n \% 2 != 0).

C = output[length = n]

A = 1st sorted array [n/2]

B = 2nd sorted array [n/2]

i = 1
\\
j = 1
\vspace{11pt}
\begin{aside}
    \textbf{Note:} we assume the starting point is 1 instead of 0.
\end{aside}
\vspace{11pt}

\begin{lstlisting}[language=Python]
for k = 1 to n  
    if A(i) < B(j)
        C(k) = A(i)
        i++
    else [B(j) < A(i)]
        C(k) = B(j)
        j++

\end{lstlisting}
\vspace{11pt}
\vspace{11pt}
\begin{aside}
    \textbf{Note:} This is just the pseudo code for merging, which is O(m). Where m $\leq$ n.
\end{aside}
\vspace{11pt}

\textbf{Running Time of Merge Sort}
\vspace{11pt}

The time complexity is O(\(nlog(n)\)). It takes \(log_2n\) times to divide an array of size n by 2 to get 1 or 0 elements.

The running time is roughly \# amount of lines of code executed.

\textbf{Upshot:} the running time of \textbf{merge} in the array of m numbers is 4m + 2 $\leq$ 6m where m $\geq$ 1

\textbf{Claim:} Merge Sort requires $\leq$ \(6n*log_2n + 6n\)  operations to sort n numbers

\vspace{11pt}
\begin{aside}
    \textbf{Recall:} \(log_2n\) is the \# of times you divide by 2 until you get down to 1.
\end{aside}
\vspace{11pt}

\textbf{Proof of claim (assuming n = power of 2)}

Use a recursive tree:

level 0 $\to$ root (outer call to Merge Sort) [\(2^1 - 1 = 1\) cumulative divisions]

level 1 $\to$ 1st recursive call [\(2^2 - 1 = 3\) cumulative divisions]

level 2 $\to$ 2nd recursive call [\(2^3 - 1 = 7\) cumulative divisions]

...

\vspace{11pt}
\begin{aside}
   \textbf{Quiz:} Roughly how many levels does this recursion tree have (as a function of $n$, the length of the input array)?

   $\to$ \(log_2n\), well actually \(log_2n + 1\) for the root level.
\end{aside}
\vspace{11pt}

\vspace{11pt}
\begin{aside}
   \textbf{Quiz:} What is the pattern? Fill in the blanks in the following statement: at each level j = 0,1,2,.., \(log_2n\), there are <blank> subproblems, each of size <blank>.

   $\to$ \(2^j\) subproblems \& each the size of \(n / 2^j\).

   \textbf{Proof of claim (assuming n = power of 2):} 
   
   $\leq$ \(2^j * 6(n/2^j) = 6n\), where $6$ is the work per level-j subproblem.

   If there is $6n$ work per level, and there are \(log_2n + 1\) levels, then the total Big-O time complexity is:

   O(\(6n(log_2n + 1)\))
\end{aside}
\vspace{11pt}

\subsection{Guiding Principles for Analysis of Algorithms}
\vspace{11pt}
\textbf{Guiding Principle \#1}
\vspace{11pt}

"Worst-case analysis": Our running time bound holds for every input of length n. This is particularly appropriate for "general-purpose" routines, as opposed to "average-case" analysis and benchmarks.

In fact, the worst-case analysis is usually easier to analyze, making our job easier.

\vspace{11pt}
\textbf{Guiding Principle \#2}
\vspace{11pt}

Constant factors and lower-order terms do not need to be paid much attention to. The justifications for this include making algorithms easier to analyze, the fact that constants depend on architecture, compiler, and/or programmer anyways, and that the reduction of constants lose only very little predictive power (as we will see).

\vspace{11pt}
\textbf{Guiding Principle \#3}
\vspace{11pt}

\textbf{Asymptotic Analysis:} Focus on running time for large input size $n$.

\vspace{11pt}
\begin{aside}
    \textbf{For example}: \(6nlog_2n + 6n\) [merge sort] may be worse than \((1/2)n^2\) [insertion sort] for very small n. This can lead to wrong conclusions that insertion sort is better, but at a small enough n, the difference in performance is extremely minimal (nanoseconds).
\end{aside}
\vspace{11pt}

Really, only big problems with large n are interesting, as different Big-O functions involving n have different growth rates.

\vspace{11pt}
\includegraphics[width=0.7\textwidth]{1.png}

\vspace{11pt}
\includegraphics[width=0.7\textwidth]{2.png}

\vspace{11pt}
\textbf{What is a "Fast" Algorithm?}
\vspace{11pt}

A fast algorithm is one where the worst case running time grows slowly with input size. An example being O(\(log_2n\)) or even O(1), although that is rare.

\vspace{11pt}
\begin{aside}
    A good goal is usually keeping an algorithm as close to \(O(n)\) as possible.
\end{aside}
\vspace{11pt}

\vspace{11pt}
\textbf{For-Free Primitives}
\vspace{11pt}

An algorithm with a linear or near-linear running time acts as a primitive that can be used essentially "for free". In other words, the amount of time required barely exceeds what you need to read the input.

An example of this can be sorting. It is important to put these for-free primitive algorithms into your toolbox.









\end{document}